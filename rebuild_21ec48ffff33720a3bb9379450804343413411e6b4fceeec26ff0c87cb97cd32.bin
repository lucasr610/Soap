#!/usr/bin/env python3
"""
web_scraper_agent_v2.py: Recursive web scraper with deduplication, session cap, and rotor integration.
Usage:
  python3 web_scraper_agent_v2.py <start_url> [--max-depth N]
"""
import sys
import os
import time
import logging
import hashlib
import requests
from urllib.parse import urljoin, urlparse
from pathlib import Path
from bs4 import BeautifulSoup

# Configuration
HOME_DIR = Path.home()
SCRAPE_DIR = HOME_DIR / "Soap" / "data" / "web_scrape_v2"
LOG_DIR = HOME_DIR / "Soap" / "data" / "logs"
VISITED_LOG = SCRAPE_DIR / "visited_urls_v2.txt"
SESSION_CAP_BYTES = 10 * 1024**3  # 10 GB
ALLOWED_EXTENSIONS = {".pdf", ".docx", ".jpeg", ".jpg", ".png", ".txt"}
DEFAULT_MAX_DEPTH = 3

# State
seen_hashes = set()
downloaded_bytes = 0
visited_urls = set()


def setup_environment():
    SCRAPE_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        filename=str(LOG_DIR / 'web_scrape_v2.log'),
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s'
    )
    if VISITED_LOG.exists():
        for line in VISITED_LOG.read_text().splitlines():
            if ' | ' in line:
                h, url = line.split(' | ', 1)
                visited_urls.add(h)


def log(msg, level=logging.INFO):
    print(msg)
    logging.log(level, msg)


def hash_url(url: str) -> str:
    return hashlib.sha256(url.encode('utf-8')).hexdigest()


def is_visited(url: str) -> bool:
    return hash_url(url) in visited_urls


def mark_visited(url: str):
    h = hash_url(url)
    visited_urls.add(h)
    with open(VISITED_LOG, 'a') as f:
        f.write(f"{h} | {url}\n")


def get_extension(url: str) -> str:
    path = urlparse(url).path
    return Path(path).suffix.lower()


def is_downloadable(url: str) -> bool:
    return get_extension(url) in ALLOWED_EXTENSIONS


def download_file(url: str):
    global downloaded_bytes
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        content = resp.content
        sha = hashlib.sha256(content).hexdigest()
        if sha in seen_hashes:
            return
        size = len(content)
        if downloaded_bytes + size > SESSION_CAP_BYTES:
            log("‚ùå Session cap reached. Stopping scraper.", logging.WARNING)
            sys.exit(0)
        seen_hashes.add(sha)
        downloaded_bytes += size
        out_path = SCRAPE_DIR / f"file_{sha}{get_extension(url)}"
        with open(out_path, 'wb') as f:
            f.write(content)
        log(f"üì• Downloaded: {out_path.name} ({size // 1024} KB)")
    except Exception as e:
        log(f"‚ö†Ô∏è Failed to download {url}: {e}", logging.ERROR)


def scrape(url: str, depth: int, max_depth: int):
    if depth > max_depth or is_visited(url):
        return
    log(f"üîç Scraping (depth {depth}): {url}")
    mark_visited(url)
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        # Save page text
        page_hash = hash_url(url)
        text_file = SCRAPE_DIR / f"page_{page_hash}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(soup.get_text())
        # Iterate links
        base_netloc = urlparse(url).netloc
        for tag in soup.find_all(['a', 'link', 'script'], href=True):
            href = tag.get('href') or ''
            link = urljoin(url, href)
            parsed = urlparse(link)
            if parsed.netloc != base_netloc:
                continue
            if is_downloadable(link):
                download_file(link)
            else:
                scrape(link, depth + 1, max_depth)
    except Exception as e:
        log(f"‚ö†Ô∏è Error scraping {url}: {e}", logging.ERROR)


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(1)
    start_url = sys.argv[1]
    max_depth = DEFAULT_MAX_DEPTH
    if len(sys.argv) == 3 and sys.argv[2].isdigit():
        max_depth = int(sys.argv[2])
    setup_environment()
    log("üöÄ Starting web_scraper_agent_v2...", logging.INFO)
    scrape(start_url, depth=0, max_depth=max_depth)
    log("‚úÖ Web scraping complete.", logging.INFO)

if __name__ == '__main__':
    main()
