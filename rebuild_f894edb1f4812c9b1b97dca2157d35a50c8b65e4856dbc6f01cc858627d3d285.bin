#!/usr/bin/env python3
import os
import hashlib
import json
import datetime
from pathlib import Path
from bs4 import BeautifulSoup

# Directories for scraped input and generated SOP outputs\ nSCRAPE_DIR = Path.home() / "Soap/data/web_scrape"
OUTPUT_DIR = Path.home() / "Soap/data/sop_outputs"
CACHE_FILE = OUTPUT_DIR / ".cache.json"
# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_cache():
    """
    Load the cache of file hashes to skip unchanged files.
    """
    if CACHE_FILE.exists():
        try:
            return json.loads(CACHE_FILE.read_text())
        except json.JSONDecodeError:
            return {}
    return {}


def save_cache(cache):
    """
    Save the updated cache back to disk.
    """
    CACHE_FILE.write_text(json.dumps(cache, indent=2))


def compute_hash(content_bytes):
    """
    Compute a SHA-256 hash of the given content.
    """
    h = hashlib.sha256()
    h.update(content_bytes)
    return h.hexdigest()


def clean_html_text(html_bytes):
    """
    Strip HTML tags from bytes and return clean text.
    """
    try:
        soup = BeautifulSoup(html_bytes, "html.parser")
        text = soup.get_text(separator="\n")
        return text.strip()
    except Exception as e:
        print(f"‚ùå Failed to parse HTML: {e}")
        return ""


def extract_sections(text):
    """
    Extract SOP sections based on simple heading keywords.
    """
    sections = {
        "title": None,
        "purpose": None,
        "scope": None,
        "tools": [],
        "materials": [],
        "safety": [],
        "procedure": [],
        "troubleshooting": [],
        "maintenance": [],
        "references": []
    }

    lines = text.splitlines()
    section = "procedure"
    for line in lines:
        l = line.strip().lower()

        # Detect section headings
        if "purpose" in l:
            section = "purpose"
        elif "scope" in l:
            section = "scope"
        elif "tool" in l:
            section = "tools"
        elif "material" in l:
            section = "materials"
        elif "safety" in l:
            section = "safety"
        elif "procedure" in l or "steps" in l:
            section = "procedure"
        elif "troubleshooting" in l:
            section = "troubleshooting"
        elif "maintenance" in l:
            section = "maintenance"
        elif "reference" in l:
            section = "references"

        # Assign lines to detected sections
        if section in ["tools", "materials", "safety", "procedure", "troubleshooting", "maintenance", "references"]:
            sections[section].append(line.strip())
        elif section in ["purpose", "scope", "title"] and not sections[section]:
            sections[section] = line.strip()

    return sections


def synthesize_sop(file_path, cache):
    """
    Generate an SOP JSON for a single file, skipping if unchanged.

    Returns:
      tuple(sop_dict or None, file_hash or None)
    """
    try:
        content = file_path.read_bytes()
    except Exception as e:
        print(f"‚ùå Cannot read {file_path.name}: {e}")
        return None, None

    file_hash = compute_hash(content)
    if cache.get(file_path.name) == file_hash:
        print(f"üîÑ Skipping unchanged: {file_path.name}")
        return None, None

    raw_text = clean_html_text(content)
    sections = extract_sections(raw_text)

    meta = {
        "source_file": file_path.name,
        "title": sections.get("title") or f"Auto-SOP from {file_path.name}",
        "version": "1.0",
        "status": "Generated",
        "generated_on": datetime.datetime.utcnow().isoformat() + "Z",
        "hash": file_hash
    }
    sop_data = {"meta": meta, "sections": sections}
    return sop_data, file_hash


def main():
    cache = load_cache()
    for file_path in SCRAPE_DIR.iterdir():
        if not file_path.is_file():
            continue
        print(f"üß† Processing: {file_path.name}")
        sop_data, new_hash = synthesize_sop(file_path, cache)
        if sop_data is None:
            continue

        output_file = OUTPUT_DIR / f"{file_path.stem}.json"
        try:
            output_file.write_text(json.dumps(sop_data, indent=2))
            print(f"‚úÖ Wrote SOP to {output_file.name}")
            cache[file_path.name] = new_hash
            save_cache(cache)
        except Exception as e:
            print(f"‚ùå Failed to write {output_file.name}: {e}")

    print("‚úÖ All done.")


if __name__ == "__main__":
    main()
