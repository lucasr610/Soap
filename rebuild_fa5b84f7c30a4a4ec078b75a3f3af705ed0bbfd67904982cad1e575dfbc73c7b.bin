#!/usr/bin/env python3
"""
Deep web scraper agent for ATI SOP system.
Recursively crawls a given website, saves allowed resources,
avoids duplicates, and logs progress.
"""
import argparse
import logging
import time
import hashlib
from pathlib import Path
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configuration defaults
DEFAULT_MAX_DEPTH = 3
DEFAULT_DELAY = 1.0  # seconds between requests
ALLOWED_EXTENSIONS = {'.pdf', '.docx', '.jpeg', '.jpg', '.png', '.txt', '.html', '.htm'}


def setup_logging(log_path: Path):
    log_path.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        filename=str(log_path),
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s'
    )


def compute_filename(url: str, base_dir: Path) -> Path:
    """Generate a unique filename based on SHA256 of URL."""
    sha = hashlib.sha256(url.encode('utf-8')).hexdigest()
    ext = Path(urlparse(url).path).suffix.lower() or '.html'
    if ext not in ALLOWED_EXTENSIONS:
        ext = '.html'
    return base_dir / f"{sha}{ext}"


def save_content(path: Path, content: bytes):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'wb') as f:
        f.write(content)
    logging.info(f"Saved: {path.name}")


def fetch_url(session: requests.Session, url: str, delay: float):
    """Fetch URL with retry and delay, return (url, response) or (url, None)."""
    try:
        time.sleep(delay)
        resp = session.get(url, timeout=10)
        if resp.status_code == 200:
            return url, resp
        logging.warning(f"Non-200 status {resp.status_code} at {url}")
    except Exception as e:
        logging.error(f"Request error {url}: {e}")
    return url, None


def crawl_site(root_url: str, base_dir: Path, max_depth: int, delay: float, max_workers: int):
    visited = set()
    session = requests.Session()
    session.headers.update({'User-Agent': 'ATI-SOP-Scraper/1.0'})
    base_domain = urlparse(root_url).netloc

    # Thread-safe executor for concurrency
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        def schedule(url, depth):
            if depth > max_depth or url in visited:
                return
            visited.add(url)
            futures[executor.submit(fetch_url, session, url, delay)] = depth

        # Start with the root URL
        schedule(root_url, 0)

        while futures:
            done, _ = as_completed(futures), None
            for future in list(futures):
                depth = futures[future]
                try:
                    url, resp = future.result()
                except Exception:
                    futures.pop(future)
                    continue
                futures.pop(future)

                if resp is None:
                    continue

                file_path = compute_filename(url, base_dir)
                save_content(file_path, resp.content)

                content_type = resp.headers.get('Content-Type', '')
                # If HTML, parse links
                if 'text/html' in content_type and depth < max_depth:
                    try:
                        soup = BeautifulSoup(resp.text, 'html.parser')
                        for tag in soup.find_all(['a', 'img'], href=True) + soup.find_all('img', src=True):
                            href = tag.get('href') or tag.get('src')
                            next_url = urljoin(url, href)
                            if urlparse(next_url).netloc == base_domain:
                                schedule(next_url, depth + 1)
                    except Exception as e:
                        logging.error(f"HTML parse error {url}: {e}")

    logging.info("Crawl complete.")


def main():
    parser = argparse.ArgumentParser(description='Deep web scraper for ATI SOP')
    parser.add_argument('root_url', help='Starting URL to crawl')
    parser.add_argument('--output', default=str(Path.home() / 'Soap/data/web_scrape'),
                        help='Directory to save files')
    parser.add_argument('--max-depth', type=int, default=DEFAULT_MAX_DEPTH,
                        help='Maximum crawl depth')
    parser.add_argument('--delay', type=float, default=DEFAULT_DELAY,
                        help='Delay between requests (seconds)')
    parser.add_argument('--workers', type=int, default=5,
                        help='Number of concurrent workers')
    args = parser.parse_args()

    base_dir = Path(args.output)
    log_file = base_dir / 'scraper.log'
    setup_logging(log_file)

    logging.info(f"Starting crawl: {args.root_url}")
    crawl_site(args.root_url, base_dir, args.max_depth, args.delay, args.workers)

if __name__ == '__main__':
    main()
